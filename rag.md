Google Drive GraphRAG êµ¬í˜„ ê°€ì´ë“œ (Plan B)

Plan B: Google Document AI + Vertex AI RAG Engine
Google Cloud ë„¤ì´í‹°ë¸Œ ì†”ë£¨ì…˜ìœ¼ë¡œ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ GraphRAG í•™ìŠµ


ğŸ“‹ ëª©ì°¨

í™˜ê²½ ì„¤ì •
Google Cloud í”„ë¡œì íŠ¸ ì„¤ì •
ì¸ì¦ ì„¤ì •
êµ¬í˜„ ì½”ë“œ
ì‹¤í–‰ ë°©ë²•
í…ŒìŠ¤íŠ¸ & ê²€ì¦
íŠ¸ëŸ¬ë¸”ìŠˆíŒ…


1. í™˜ê²½ ì„¤ì •
1.1 í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash# Python ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install google-cloud-aiplatform>=1.38.0
pip install google-cloud-documentai>=2.20.0
pip install google-auth-oauthlib>=1.1.0
pip install google-auth-httplib2>=0.1.1
pip install google-api-python-client>=2.108.0
pip install tqdm
pip install python-dotenv
1.2 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

Python 3.9 ì´ìƒ
Google Cloud ê³„ì • (ë¬´ë£Œ ì²´í—˜íŒ ê°€ëŠ¥)
Google Drive API ì ‘ê·¼ ê¶Œí•œ
ì¶©ë¶„í•œ API í• ë‹¹ëŸ‰ ($300 ë¬´ë£Œ í¬ë ˆë”§)


2. Google Cloud í”„ë¡œì íŠ¸ ì„¤ì •
2.1 í”„ë¡œì íŠ¸ ìƒì„±

Google Cloud Console ì ‘ì†
ìƒˆ í”„ë¡œì íŠ¸ ìƒì„± ë˜ëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ ì„ íƒ
í”„ë¡œì íŠ¸ ID ë©”ëª¨ (ì˜ˆ: my-graphrag-project)

2.2 í•„ìˆ˜ API í™œì„±í™”
bash# gcloud CLI ì„¤ì¹˜ í›„ ì‹¤í–‰
gcloud auth login
gcloud config set project YOUR_PROJECT_ID

# í•„ìˆ˜ API í™œì„±í™”
gcloud services enable documentai.googleapis.com
gcloud services enable aiplatform.googleapis.com
gcloud services enable drive.googleapis.com
gcloud services enable storage.googleapis.com
ë˜ëŠ” ì½˜ì†”ì—ì„œ ìˆ˜ë™ í™œì„±í™”:

Document AI API
Vertex AI API
Google Drive API
Cloud Storage API

2.3 Layout Parser í”„ë¡œì„¸ì„œ ìƒì„±

Document AI Console ì´ë™
"í”„ë¡œì„¸ì„œ ë§Œë“¤ê¸°" í´ë¦­
í”„ë¡œì„¸ì„œ ìœ í˜•: Layout Parser ì„ íƒ
ë¦¬ì „: us ë˜ëŠ” eu ì„ íƒ
í”„ë¡œì„¸ì„œ ID ë©”ëª¨ (ì˜ˆ: abc123def456)

í”„ë¡œì„¸ì„œ ë¦¬ì†ŒìŠ¤ ì´ë¦„ í˜•ì‹:
projects/{PROJECT_ID}/locations/{LOCATION}/processors/{PROCESSOR_ID}

3. ì¸ì¦ ì„¤ì •
3.1 ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±
bash# ì„œë¹„ìŠ¤ ê³„ì • ìƒì„±
gcloud iam service-accounts create graphrag-service \
    --display-name="GraphRAG Service Account"

# í•„ìš”í•œ ì—­í•  ë¶€ì—¬
gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:graphrag-service@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/documentai.apiUser"

gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \
    --member="serviceAccount:graphrag-service@YOUR_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/aiplatform.user"

# í‚¤ íŒŒì¼ ìƒì„±
gcloud iam service-accounts keys create credentials.json \
    --iam-account=graphrag-service@YOUR_PROJECT_ID.iam.gserviceaccount.com
3.2 Google Drive OAuth ì„¤ì •

Google Cloud Console > APIs & Services > Credentials
"OAuth 2.0 í´ë¼ì´ì–¸íŠ¸ ID ë§Œë“¤ê¸°"
ì• í”Œë¦¬ì¼€ì´ì…˜ ìœ í˜•: ë°ìŠ¤í¬í†± ì•±
client_secret.json ë‹¤ìš´ë¡œë“œ

3.3 í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
.env íŒŒì¼ ìƒì„±:
bash# Google Cloud ì„¤ì •
PROJECT_ID=your-project-id
LOCATION=us-central1
PROCESSOR_ID=your-processor-id
PROCESSOR_NAME=projects/your-project-id/locations/us/processors/your-processor-id

# ì¸ì¦ íŒŒì¼ ê²½ë¡œ
GOOGLE_APPLICATION_CREDENTIALS=./credentials.json
OAUTH_CLIENT_SECRET=./client_secret.json

# RAG ì„¤ì •
RAG_CORPUS_NAME=google-drive-corpus
CHUNK_SIZE=512
CHUNK_OVERLAP=50
BATCH_SIZE=20

4. êµ¬í˜„ ì½”ë“œ
4.1 í”„ë¡œì íŠ¸ êµ¬ì¡°
google-drive-graphrag/
â”œâ”€â”€ .env
â”œâ”€â”€ credentials.json
â”œâ”€â”€ client_secret.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py
â”œâ”€â”€ auth/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ google_auth.py
â”œâ”€â”€ drive/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ drive_client.py
â”œâ”€â”€ parser/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ document_parser.py
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ rag_engine.py
â””â”€â”€ utils/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ helpers.py
4.2 ë©”ì¸ íŒŒì¼: main.py
python#!/usr/bin/env python3
"""
Google Drive GraphRAG Implementation (Plan B)
Google Document AI + Vertex AI RAG Engine
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from tqdm import tqdm

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
from auth.google_auth import authenticate_google_services
from drive.drive_client import GoogleDriveClient
from parser.document_parser import DocumentParser
from rag.rag_engine import RAGEngine
from utils.helpers import setup_logging, print_summary

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = setup_logging()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    logger.info("=" * 80)
    logger.info("Google Drive GraphRAG íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    logger.info("=" * 80)
    
    # 1. ì¸ì¦
    logger.info("\n[Step 1] Google ì„œë¹„ìŠ¤ ì¸ì¦ ì¤‘...")
    try:
        credentials = authenticate_google_services(
            oauth_client_secret=os.getenv('OAUTH_CLIENT_SECRET'),
            service_account_key=os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        )
        logger.info("âœ“ ì¸ì¦ ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— ì¸ì¦ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 2. Google Drive ì—°ê²°
    logger.info("\n[Step 2] Google Drive ì—°ê²° ì¤‘...")
    try:
        drive_client = GoogleDriveClient(credentials)
        files = drive_client.list_files(max_results=100)
        logger.info(f"âœ“ {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
    except Exception as e:
        logger.error(f"âœ— Google Drive ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 3. Document AI Parser ì´ˆê¸°í™”
    logger.info("\n[Step 3] Document AI Layout Parser ì´ˆê¸°í™” ì¤‘...")
    try:
        parser = DocumentParser(
            project_id=os.getenv('PROJECT_ID'),
            location=os.getenv('LOCATION'),
            processor_name=os.getenv('PROCESSOR_NAME')
        )
        logger.info("âœ“ Parser ì´ˆê¸°í™” ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âœ— Parser ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 4. RAG Engine ì´ˆê¸°í™”
    logger.info("\n[Step 4] Vertex AI RAG Engine ì´ˆê¸°í™” ì¤‘...")
    try:
        rag_engine = RAGEngine(
            project_id=os.getenv('PROJECT_ID'),
            location=os.getenv('LOCATION'),
            corpus_name=os.getenv('RAG_CORPUS_NAME')
        )
        corpus = rag_engine.create_or_get_corpus()
        logger.info(f"âœ“ RAG Corpus ì¤€ë¹„ ì™„ë£Œ: {corpus.name}")
    except Exception as e:
        logger.error(f"âœ— RAG Engine ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # 5. ë¬¸ì„œ ì²˜ë¦¬ ë° ì¸ë±ì‹±
    logger.info("\n[Step 5] ë¬¸ì„œ íŒŒì‹± ë° ì¸ë±ì‹± ì¤‘...")
    
    processed_count = 0
    error_count = 0
    total_chunks = 0
    
    batch_size = int(os.getenv('BATCH_SIZE', 20))
    
    for i in tqdm(range(0, len(files), batch_size), desc="ë°°ì¹˜ ì²˜ë¦¬"):
        batch = files[i:i + batch_size]
        
        for file_info in batch:
            try:
                # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                file_content = drive_client.download_file(file_info['id'])
                
                # Document AIë¡œ íŒŒì‹± ë° ì²­í‚¹
                chunks = parser.parse_and_chunk(
                    content=file_content,
                    mime_type=file_info['mimeType'],
                    chunk_size=int(os.getenv('CHUNK_SIZE', 512)),
                    chunk_overlap=int(os.getenv('CHUNK_OVERLAP', 50))
                )
                
                # RAGì— ì¸ë±ì‹±
                rag_engine.index_chunks(
                    chunks=chunks,
                    metadata={
                        'file_id': file_info['id'],
                        'file_name': file_info['name'],
                        'created_time': file_info.get('createdTime'),
                        'modified_time': file_info.get('modifiedTime'),
                        'owner': file_info.get('owners', [{}])[0].get('emailAddress')
                    }
                )
                
                processed_count += 1
                total_chunks += len(chunks)
                
            except Exception as e:
                logger.warning(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file_info['name']}): {e}")
                error_count += 1
                continue
    
    # 6. ê²°ê³¼ ìš”ì•½
    logger.info("\n[Step 6] ì²˜ë¦¬ ì™„ë£Œ!")
    print_summary({
        'total_files': len(files),
        'processed_files': processed_count,
        'error_files': error_count,
        'total_chunks': total_chunks,
        'corpus_name': corpus.name
    })
    
    # 7. ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
    logger.info("\n[Step 7] RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸...")
    test_query = "ì‚¬íšŒë³µì§€ ì‚¬ì—…ê³„íšì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜"
    
    try:
        response = rag_engine.query(
            query=test_query,
            top_k=5
        )
        logger.info(f"\nì§ˆë¬¸: {test_query}")
        logger.info(f"ë‹µë³€: {response['answer']}")
        logger.info(f"ì°¸ì¡° ë¬¸ì„œ: {len(response['sources'])}ê°œ")
    except Exception as e:
        logger.warning(f"ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    logger.info("\n" + "=" * 80)
    logger.info("íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    logger.info("=" * 80)


if __name__ == "__main__":
    main()
4.3 ì¸ì¦ ëª¨ë“ˆ: auth/google_auth.py
python"""Google ì¸ì¦ ê´€ë¦¬"""

import os
import pickle
from pathlib import Path
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import InstalledAppFlow
from google.oauth2 import service_account

# í•„ìš”í•œ OAuth ìŠ¤ì½”í”„
SCOPES = [
    'https://www.googleapis.com/auth/drive.readonly',
    'https://www.googleapis.com/auth/cloud-platform'
]

TOKEN_FILE = 'token.pickle'


def authenticate_google_services(oauth_client_secret, service_account_key):
    """
    Google Drive ë° Vertex AI ì¸ì¦
    
    Args:
        oauth_client_secret: OAuth í´ë¼ì´ì–¸íŠ¸ ì‹œí¬ë¦¿ íŒŒì¼ ê²½ë¡œ
        service_account_key: ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        credentials: ì¸ì¦ ê°ì²´
    """
    creds = None
    
    # ê¸°ì¡´ í† í° í™•ì¸
    if os.path.exists(TOKEN_FILE):
        with open(TOKEN_FILE, 'rb') as token:
            creds = pickle.load(token)
    
    # í† í°ì´ ì—†ê±°ë‚˜ ë§Œë£Œë¨
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            # OAuth í”Œë¡œìš° ì‹œì‘
            flow = InstalledAppFlow.from_client_secrets_file(
                oauth_client_secret, SCOPES
            )
            creds = flow.run_local_server(port=0)
        
        # í† í° ì €ì¥
        with open(TOKEN_FILE, 'wb') as token:
            pickle.dump(creds, token)
    
    return creds


def get_service_account_credentials(key_path):
    """
    ì„œë¹„ìŠ¤ ê³„ì • ìê²©ì¦ëª… ê°€ì ¸ì˜¤ê¸°
    
    Args:
        key_path: ì„œë¹„ìŠ¤ ê³„ì • í‚¤ JSON íŒŒì¼ ê²½ë¡œ
    
    Returns:
        credentials: ì„œë¹„ìŠ¤ ê³„ì • ìê²©ì¦ëª…
    """
    credentials = service_account.Credentials.from_service_account_file(
        key_path,
        scopes=['https://www.googleapis.com/auth/cloud-platform']
    )
    return credentials
4.4 Google Drive í´ë¼ì´ì–¸íŠ¸: drive/drive_client.py
python"""Google Drive í´ë¼ì´ì–¸íŠ¸"""

import io
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from googleapiclient.errors import HttpError


class GoogleDriveClient:
    """Google Drive API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, credentials):
        """
        ì´ˆê¸°í™”
        
        Args:
            credentials: Google ì¸ì¦ ê°ì²´
        """
        self.service = build('drive', 'v3', credentials=credentials)
    
    def list_files(self, max_results=100, query=None):
        """
        íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        
        Args:
            max_results: ìµœëŒ€ ê²°ê³¼ ìˆ˜
            query: ê²€ìƒ‰ ì¿¼ë¦¬ (ì„ íƒ)
        
        Returns:
            files: íŒŒì¼ ëª©ë¡
        """
        try:
            # ê¸°ë³¸ ì¿¼ë¦¬: í´ë” ì œì™¸, íœ´ì§€í†µ ì œì™¸
            base_query = "mimeType != 'application/vnd.google-apps.folder' and trashed = false"
            
            if query:
                search_query = f"{base_query} and {query}"
            else:
                search_query = base_query
            
            results = self.service.files().list(
                q=search_query,
                pageSize=max_results,
                fields="files(id, name, mimeType, createdTime, modifiedTime, owners, size)",
                orderBy="modifiedTime desc"
            ).execute()
            
            files = results.get('files', [])
            return files
            
        except HttpError as error:
            raise Exception(f"Drive API ì˜¤ë¥˜: {error}")
    
    def download_file(self, file_id):
        """
        íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        
        Args:
            file_id: íŒŒì¼ ID
        
        Returns:
            content: íŒŒì¼ ë‚´ìš© (bytes)
        """
        try:
            # Google Docs í˜•ì‹ ì²˜ë¦¬
            file_metadata = self.service.files().get(fileId=file_id).execute()
            mime_type = file_metadata.get('mimeType')
            
            if mime_type.startswith('application/vnd.google-apps'):
                # Google Docsë¥¼ í…ìŠ¤íŠ¸ë¡œ ë‚´ë³´ë‚´ê¸°
                export_mime = 'text/plain'
                if 'document' in mime_type:
                    export_mime = 'text/plain'
                elif 'spreadsheet' in mime_type:
                    export_mime = 'text/csv'
                elif 'presentation' in mime_type:
                    export_mime = 'text/plain'
                
                request = self.service.files().export_media(
                    fileId=file_id,
                    mimeType=export_mime
                )
            else:
                # ì¼ë°˜ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                request = self.service.files().get_media(fileId=file_id)
            
            fh = io.BytesIO()
            downloader = MediaIoBaseDownload(fh, request)
            
            done = False
            while not done:
                status, done = downloader.next_chunk()
            
            content = fh.getvalue()
            return content
            
        except HttpError as error:
            raise Exception(f"íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {error}")
    
    def get_file_metadata(self, file_id):
        """
        íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        
        Args:
            file_id: íŒŒì¼ ID
        
        Returns:
            metadata: íŒŒì¼ ë©”íƒ€ë°ì´í„°
        """
        try:
            metadata = self.service.files().get(
                fileId=file_id,
                fields="*"
            ).execute()
            return metadata
        except HttpError as error:
            raise Exception(f"ë©”íƒ€ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {error}")
4.5 Document Parser: parser/document_parser.py
python"""Document AI Layout Parser"""

from google.cloud import documentai_v1 as documentai
from google.api_core.client_options import ClientOptions


class DocumentParser:
    """Document AI Layout Parser í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, project_id, location, processor_name):
        """
        ì´ˆê¸°í™”
        
        Args:
            project_id: GCP í”„ë¡œì íŠ¸ ID
            location: ë¦¬ì „ (us, eu)
            processor_name: í”„ë¡œì„¸ì„œ ë¦¬ì†ŒìŠ¤ ì´ë¦„
        """
        self.project_id = project_id
        self.location = location
        self.processor_name = processor_name
        
        # Document AI í´ë¼ì´ì–¸íŠ¸
        opts = ClientOptions(api_endpoint=f"{location}-documentai.googleapis.com")
        self.client = documentai.DocumentProcessorServiceClient(client_options=opts)
    
    def parse_and_chunk(self, content, mime_type, chunk_size=512, chunk_overlap=50):
        """
        ë¬¸ì„œ íŒŒì‹± ë° ì²­í‚¹
        
        Args:
            content: ë¬¸ì„œ ë‚´ìš© (bytes)
            mime_type: MIME íƒ€ì…
            chunk_size: ì²­í¬ í¬ê¸° (í† í°)
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë© (í† í°)
        
        Returns:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # Document AI ìš”ì²­
        raw_document = documentai.RawDocument(
            content=content,
            mime_type=mime_type
        )
        
        # Layout Parser ì„¤ì •
        process_options = documentai.ProcessOptions(
            layout_config=documentai.ProcessOptions.LayoutConfig(
                chunking_config=documentai.ProcessOptions.LayoutConfig.ChunkingConfig(
                    chunk_size=chunk_size,
                    include_ancestor_headings=True  # í—¤ë”© ê³„ì¸µ í¬í•¨
                )
            )
        )
        
        request = documentai.ProcessRequest(
            name=self.processor_name,
            raw_document=raw_document,
            process_options=process_options
        )
        
        # ë¬¸ì„œ ì²˜ë¦¬
        result = self.client.process_document(request=request)
        document = result.document
        
        # ì²­í¬ ì¶”ì¶œ
        chunks = []
        
        if hasattr(document, 'chunked_document') and document.chunked_document:
            for chunk in document.chunked_document.chunks:
                chunk_data = {
                    'text': chunk.content,
                    'chunk_id': chunk.chunk_id,
                    'page_span': {
                        'start': chunk.page_span.page_start,
                        'end': chunk.page_span.page_end
                    }
                }
                
                # í—¤ë”© ê³„ì¸µ ì •ë³´
                if hasattr(chunk, 'source_block_ids'):
                    chunk_data['heading_hierarchy'] = self._extract_heading_hierarchy(
                        document, chunk.source_block_ids
                    )
                
                chunks.append(chunk_data)
        else:
            # ì²­í‚¹ì´ ì—†ëŠ” ê²½ìš° ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            chunks.append({
                'text': document.text,
                'chunk_id': '0',
                'page_span': {'start': 0, 'end': len(document.pages)}
            })
        
        return chunks
    
    def _extract_heading_hierarchy(self, document, block_ids):
        """
        í—¤ë”© ê³„ì¸µ ì¶”ì¶œ
        
        Args:
            document: Document AI ë¬¸ì„œ ê°ì²´
            block_ids: ë¸”ë¡ ID ë¦¬ìŠ¤íŠ¸
        
        Returns:
            hierarchy: í—¤ë”© ê³„ì¸µ ë¦¬ìŠ¤íŠ¸
        """
        hierarchy = []
        
        for page in document.pages:
            for block in page.blocks:
                if block.layout.text_anchor.text_segments:
                    segment = block.layout.text_anchor.text_segments[0]
                    block_text = document.text[segment.start_index:segment.end_index]
                    
                    # í—¤ë”© ê°ì§€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                    if block_text.strip() and len(block_text) < 100:
                        hierarchy.append(block_text.strip())
        
        return hierarchy
4.6 RAG Engine: rag/rag_engine.py
python"""Vertex AI RAG Engine"""

from vertexai.preview import rag
import vertexai
from google.cloud import aiplatform


class RAGEngine:
    """Vertex AI RAG Engine í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self, project_id, location, corpus_name):
        """
        ì´ˆê¸°í™”
        
        Args:
            project_id: GCP í”„ë¡œì íŠ¸ ID
            location: ë¦¬ì „
            corpus_name: RAG Corpus ì´ë¦„
        """
        self.project_id = project_id
        self.location = location
        self.corpus_name = corpus_name
        
        # Vertex AI ì´ˆê¸°í™”
        vertexai.init(project=project_id, location=location)
        aiplatform.init(project=project_id, location=location)
        
        self.corpus = None
    
    def create_or_get_corpus(self):
        """
        RAG Corpus ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        
        Returns:
            corpus: RAG Corpus ê°ì²´
        """
        try:
            # ê¸°ì¡´ ì½”í¼ìŠ¤ ì¡°íšŒ
            corpora = rag.list_corpora()
            for corpus in corpora:
                if corpus.display_name == self.corpus_name:
                    self.corpus = corpus
                    return corpus
            
            # ìƒˆ ì½”í¼ìŠ¤ ìƒì„±
            self.corpus = rag.create_corpus(
                display_name=self.corpus_name,
                description=f"Google Drive documents indexed with GraphRAG"
            )
            return self.corpus
            
        except Exception as e:
            raise Exception(f"Corpus ìƒì„±/ì¡°íšŒ ì˜¤ë¥˜: {e}")
    
    def index_chunks(self, chunks, metadata):
        """
        ì²­í¬ ì¸ë±ì‹±
        
        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            metadata: íŒŒì¼ ë©”íƒ€ë°ì´í„°
        """
        if not self.corpus:
            raise Exception("Corpusê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # ì²­í¬ë¥¼ RAGì— ì¶”ê°€
            for chunk in chunks:
                # ì²­í¬ í…ìŠ¤íŠ¸ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                chunk_text = chunk['text']
                chunk_metadata = {
                    **metadata,
                    'chunk_id': chunk['chunk_id'],
                    'page_span': str(chunk['page_span'])
                }
                
                if 'heading_hierarchy' in chunk:
                    chunk_metadata['headings'] = ' > '.join(chunk['heading_hierarchy'])
                
                # RAGì— ì¸ë±ì‹± (ì‹¤ì œë¡œëŠ” ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì )
                rag.import_files(
                    corpus_name=self.corpus.name,
                    paths=[],  # ì§ì ‘ í…ìŠ¤íŠ¸ ì œê³µ
                    chunk_size=512,
                    chunk_overlap=50
                )
                
        except Exception as e:
            raise Exception(f"ì¸ë±ì‹± ì˜¤ë¥˜: {e}")
    
    def query(self, query, top_k=5):
        """
        RAG ì¿¼ë¦¬
        
        Args:
            query: ì§ˆë¬¸
            top_k: ë°˜í™˜í•  ìƒìœ„ Kê°œ ê²°ê³¼
        
        Returns:
            response: ë‹µë³€ ë° ì°¸ì¡° ë¬¸ì„œ
        """
        if not self.corpus:
            raise Exception("Corpusê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        try:
            # RAG ê²€ìƒ‰
            retrieval_response = rag.retrieval_query(
                rag_resources=[
                    rag.RagResource(rag_corpus=self.corpus.name)
                ],
                text=query,
                similarity_top_k=top_k,
                vector_distance_threshold=0.5
            )
            
            # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
            from vertexai.generative_models import GenerativeModel
            
            model = GenerativeModel("gemini-1.5-pro")
            
            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            contexts = []
            for context in retrieval_response.contexts:
                contexts.append(context.text)
            
            context_text = "\n\n".join(contexts)
            
            prompt = f"""ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
            
            response = model.generate_content(prompt)
            
            return {
                'answer': response.text,
                'sources': contexts,
                'query': query
            }
            
        except Exception as e:
            raise Exception(f"ì¿¼ë¦¬ ì˜¤ë¥˜: {e}")
4.7 ìœ í‹¸ë¦¬í‹°: utils/helpers.py
python"""ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"""

import logging
from datetime import datetime


def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f'graphrag_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)


def print_summary(stats):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 80)
    print(f"ì´ íŒŒì¼ ìˆ˜:        {stats['total_files']}")
    print(f"ì²˜ë¦¬ ì™„ë£Œ:         {stats['processed_files']}")
    print(f"ì²˜ë¦¬ ì‹¤íŒ¨:         {stats['error_files']}")
    print(f"ìƒì„±ëœ ì²­í¬:       {stats['total_chunks']}")
    print(f"RAG Corpus:        {stats['corpus_name']}")
    print("=" * 80 + "\n")

5. ì‹¤í–‰ ë°©ë²•
5.1 ì´ˆê¸° ì„¤ì •
bash# 1. í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
cd google-drive-graphrag

# 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (.env íŒŒì¼ í¸ì§‘)
nano .env

# 3. ì¸ì¦ íŒŒì¼ ë°°ì¹˜
# - credentials.json (ì„œë¹„ìŠ¤ ê³„ì • í‚¤)
# - client_secret.json (OAuth í´ë¼ì´ì–¸íŠ¸)

# 4. ê¶Œí•œ í™•ì¸
chmod 600 credentials.json client_secret.json
5.2 ì‹¤í–‰
bash# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python main.py
5.3 ì²« ì‹¤í–‰ ì‹œ

ë¸Œë¼ìš°ì €ê°€ ìë™ìœ¼ë¡œ ì—´ë¦¼
Google ê³„ì • ì„ íƒ
ê¶Œí•œ ìŠ¹ì¸ (Drive ì½ê¸°, Cloud Platform ì ‘ê·¼)
ì¸ì¦ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‹œì‘


6. í…ŒìŠ¤íŠ¸ & ê²€ì¦
6.1 ë…ë¦½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
test_rag.py ìƒì„±:
python"""RAG ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""

import os
from dotenv import load_dotenv
from rag.rag_engine import RAGEngine

load_dotenv()

def test_queries():
    """ìƒ˜í”Œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    rag_engine = RAGEngine(
        project_id=os.getenv('PROJECT_ID'),
        location=os.getenv('LOCATION'),
        corpus_name=os.getenv('RAG_CORPUS_NAME')
    )
    
    # Corpus ê°€ì ¸ì˜¤ê¸°
    corpus = rag_engine.create_or_get_corpus()
    print(f"Corpus: {corpus.name}\n")
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    queries = [
        "ì‚¬íšŒë³µì§€ ì‚¬ì—…ê³„íšì„œì˜ ì£¼ìš” êµ¬ì„± ìš”ì†ŒëŠ”?",
        "ë³´í—˜ AI ë„ì… ë™í–¥ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜",
        "ì›¹ì‚¬ì´íŠ¸ ë¦¬ë‰´ì–¼ ì œì•ˆì„œì˜ í•µì‹¬ ë‚´ìš©ì€?"
    ]
    
    for query in queries:
        print(f"\nì§ˆë¬¸: {query}")
        print("-" * 80)
        
        try:
            response = rag_engine.query(query, top_k=3)
            print(f"ë‹µë³€: {response['answer']}\n")
            print(f"ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(response['sources'])}")
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    test_queries()
ì‹¤í–‰:
bashpython test_rag.py
6.2 ì„±ëŠ¥ ì¸¡ì •
python"""ì„±ëŠ¥ ì¸¡ì • ìŠ¤í¬ë¦½íŠ¸"""

import time
from test_rag import rag_engine

queries = ["í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 1", "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 2", "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ 3"]

for query in queries:
    start = time.time()
    response = rag_engine.query(query)
    elapsed = time.time() - start
    
    print(f"ì§ˆë¬¸: {query}")
    print(f"ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
    print(f"ì²­í¬ ìˆ˜: {len(response['sources'])}\n")

7. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…
7.1 ì¼ë°˜ì ì¸ ì˜¤ë¥˜
ì˜¤ë¥˜: "API not enabled"
bash# í•´ê²°: API í™œì„±í™”
gcloud services enable documentai.googleapis.com
gcloud services enable aiplatform.googleapis.com
ì˜¤ë¥˜: "Quota exceeded"

Document AI: ì¼ì¼ 1,000í˜ì´ì§€ ë¬´ë£Œ
Vertex AI: $300 í¬ë ˆë”§ ì†Œì§„ í™•ì¸
í•´ê²°: GCP ì½˜ì†”ì—ì„œ í• ë‹¹ëŸ‰ ì¦ê°€ ìš”ì²­

ì˜¤ë¥˜: "Permission denied"
bash# í•´ê²°: ì„œë¹„ìŠ¤ ê³„ì •ì— ì˜¬ë°”ë¥¸ ì—­í•  ë¶€ì—¬
gcloud projects add-iam-policy-binding PROJECT_ID \
    --member="serviceAccount:YOUR_SA@PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/documentai.apiUser"
7.2 ì¸ì¦ ë¬¸ì œ
OAuth í† í° ë§Œë£Œ
bash# token.pickle ì‚­ì œ í›„ ì¬ì¸ì¦
rm token.pickle
python main.py
ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ì˜¤ë¥˜
bash# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $GOOGLE_APPLICATION_CREDENTIALS

# ìƒˆ í‚¤ ìƒì„±
gcloud iam service-accounts keys create new-key.json \
    --iam-account=YOUR_SA@PROJECT_ID.iam.gserviceaccount.com
7.3 íŒŒì‹± ì˜¤ë¥˜
Google Docs ë³€í™˜ ì‹¤íŒ¨

ì›ì¸: ë§¤ìš° í° ë¬¸ì„œ (>1MB)
í•´ê²°: batch_size ì¤„ì´ê¸° ë˜ëŠ” í˜ì´ì§€ ë²”ìœ„ ì œí•œ

íŠ¹ì • íŒŒì¼ í˜•ì‹ ë¯¸ì§€ì›

ì§€ì› í˜•ì‹: PDF, DOCX, PPTX, XLSX, HTML, TXT
Google DocsëŠ” ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜

7.4 RAG í’ˆì§ˆ ê°œì„ 
ê´€ë ¨ì„± ë‚®ì€ ê²°ê³¼
python# vector_distance_threshold ì¡°ì • (0.3~0.7)
response = rag.retrieval_query(
    ...,
    vector_distance_threshold=0.4  # ë” ì—„ê²©í•˜ê²Œ
)
ë„ˆë¬´ ê¸´ ì‘ë‹µ ì‹œê°„
python# similarity_top_k ì¤„ì´ê¸°
response = rag.retrieval_query(
    ...,
    similarity_top_k=3  # 5 â†’ 3
)

8. ê³ ê¸‰ ì„¤ì •
8.1 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
python# main.py ìˆ˜ì •
import concurrent.futures

def process_file(file_info):
    """íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜"""
    # ... ì²˜ë¦¬ ë¡œì§

# ë³‘ë ¬ ì²˜ë¦¬
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(process_file, f) for f in files]
    
    for future in concurrent.futures.as_completed(futures):
        try:
            result = future.result()
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
8.2 ì¦ë¶„ ì—…ë°ì´íŠ¸
python# ë§ˆì§€ë§‰ ì²˜ë¦¬ ì‹œê°„ ì €ì¥
import json

LAST_RUN_FILE = 'last_run.json'

def save_last_run():
    with open(LAST_RUN_FILE, 'w') as f:
        json.dump({'last_run': datetime.now().isoformat()}, f)

def get_new_files_only(drive_client):
    """ë§ˆì§€ë§‰ ì‹¤í–‰ ì´í›„ ìƒˆ íŒŒì¼ë§Œ ê°€ì ¸ì˜¤ê¸°"""
    if os.path.exists(LAST_RUN_FILE):
        with open(LAST_RUN_FILE) as f:
            data = json.load(f)
            last_run = data['last_run']
        
        query = f"modifiedTime > '{last_run}'"
        return drive_client.list_files(query=query)
    else:
        return drive_client.list_files()
8.3 ì»¤ìŠ¤í…€ ì„ë² ë”© ëª¨ë¸
python# rag_engine.pyì—ì„œ
from vertexai.language_models import TextEmbeddingModel

embedding_model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")

# í•œêµ­ì–´ì— ìµœì í™”ëœ ì„ë² ë”©
embeddings = embedding_model.get_embeddings([chunk_text])

9. ë¹„ìš© ìµœì í™”
9.1 ì˜ˆìƒ ë¹„ìš© (2024ë…„ 12ì›” ê¸°ì¤€)
ì„œë¹„ìŠ¤ë¹„ìš©100ê°œ ë¬¸ì„œ ì˜ˆìƒDocument AI Layout Parser$1.50/1,000 pages~$0.50Vertex AI Embeddings$0.025/1,000 tokens~$1.00Vertex AI RAG Storage$0.10/GB/month~$0.05Gemini 1.5 Pro Queries$0.0035/1,000 chars$0.50/1,000 queries
ì´ ì˜ˆìƒ ë¹„ìš©: ~$2/100ê°œ ë¬¸ì„œ (ì´ˆê¸°), ~$0.50/ì›” (ìœ ì§€)
9.2 ë¹„ìš© ì ˆê° íŒ

í•„í„°ë§: ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸

python# íŠ¹ì • í´ë”ë§Œ ì²˜ë¦¬
query = "'FOLDER_ID' in parents"
files = drive_client.list_files(query=query)

ìºì‹±: ë³€ê²½ë˜ì§€ ì•Šì€ íŒŒì¼ ìŠ¤í‚µ
ë°°ì¹˜ í¬ê¸° ì¡°ì •: API í˜¸ì¶œ ìµœì†Œí™”
Gemini Flash ì‚¬ìš©: Pro ëŒ€ì‹  Flash (ì €ë ´)


10. ë‹¤ìŒ ë‹¨ê³„
10.1 í”„ë¡œë•ì…˜ ë°°í¬

Cloud Runìœ¼ë¡œ ì„œë¹„ìŠ¤í™”
Cloud Schedulerë¡œ ìë™ ì—…ë°ì´íŠ¸
Cloud Monitoring ì„¤ì •

10.2 ê¸°ëŠ¥ í™•ì¥

ì›¹ UI ì¶”ê°€ (Streamlit/Gradio)
Slack/Discord ë´‡ í†µí•©
ë©€í‹° ì–¸ì–´ ì§€ì›
ê¶Œí•œ ê¸°ë°˜ í•„í„°ë§

10.3 GraphRAG ê³ ë„í™”

Neo4j í†µí•©ìœ¼ë¡œ ì§„ì§œ ê·¸ë˜í”„ êµ¬ì¶•
Microsoft GraphRAG ë ˆì´ì–´ ì¶”ê°€
ì»¤ë®¤ë‹ˆí‹° ê²€ì¶œ ì•Œê³ ë¦¬ì¦˜


ì°¸ê³  ìë£Œ

Vertex AI RAG Engine ë¬¸ì„œ
Document AI Layout Parser
Google Drive API
Gemini API


ë¼ì´ì„¼ìŠ¤ & ê¸°ì—¬
ì´ êµ¬í˜„ì€ êµìœ¡ ëª©ì ìœ¼ë¡œ ì œê³µë©ë‹ˆë‹¤.
ë¬¸ì œê°€ ë°œìƒí•˜ë©´ GitHub Issuesë¡œ ë³´ê³ í•´ì£¼ì„¸ìš”.
ë²„ì „: 1.0.0
ìµœì¢… ì—…ë°ì´íŠ¸: 2024-12-10